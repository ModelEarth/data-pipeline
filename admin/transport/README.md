# Data Transport Scripts

This directory contains scripts for managing CSV file transportation and tracking in the data-pipeline repository.

## Overview

The data-pipeline project generates CSV files that are stored in different locations:

1. **Local outputs** - CSV files generated within the data-pipeline repo
2. **External repos** - CSV files written to sibling repositories:
   - `community-data` - Industry and geographic data files
   - `community-timelines` - Time-series data files

These scripts help manage both types of outputs and keep the `nodes.csv` metadata file up to date.

## Scripts

### 1. `data_transport_enhanced.py`

**Purpose**: Transports CSV files from data-pipeline to external storage repo (data-pipe-csv) and updates nodes.csv with actual file sizes.

**Features**:
- Copies CSV files to GitHub repo or local directory
- Updates `nodes.csv` with actual file sizes and counts for local outputs
- Respects retention rules for lookup/crosswalk files
- Generates detailed transport reports
- Supports Git LFS for large files

**Usage**:
```bash
# Transport to GitHub (default mode)
python admin/transport/data_transport_enhanced.py

# Transport to local directory only
# (Edit OUTPUT_LOCAL_PATH in script first)
python admin/transport/data_transport_enhanced.py
```

**What it tracks**: CSV files generated locally within the data-pipeline repo.

**What it updates in nodes.csv**:
- `actual_size_mb` - Total size of CSV files in MB
- `csv_file_count` - Number of CSV files generated
- `last_transport_date` - Date of last transport

**Retention rules** (files NOT transported):
- `nodes.csv` - The metadata file itself
- Files matching `*crosswalk*.csv`, `*_to_*.csv` - Lookup tables
- Files matching `*fips*.csv`, `*id_list*.csv` - ID lists and codes
- Files in `timelines/prep/all/input/` - Input data for ML/regression
- Files matching `*lookup*.csv` - Lookup tables

### 2. `scan_external_repos.py` ⭐ NEW

**Purpose**: Scans external repositories (community-data, community-timelines) to find CSV files generated by data-pipeline scripts and updates nodes.csv.

**Features**:
- Handles relative paths like `../../../../community-timelines/...`
- Resolves paths from the context of where each script runs
- Gracefully handles missing repos (warns but doesn't crash)
- Updates nodes.csv with external repo file sizes and counts
- Supports scripts and Jupyter notebooks
- Dry-run mode for testing
- Verbose mode for debugging

**Usage**:
```bash
# Scan external repos and update nodes.csv
python admin/transport/scan_external_repos.py

# Dry-run mode (show what would be updated without saving)
python admin/transport/scan_external_repos.py --dry-run

# Verbose mode (show detailed output)
python admin/transport/scan_external_repos.py --verbose

# Dry-run + verbose
python admin/transport/scan_external_repos.py --dry-run --verbose

# Help
python admin/transport/scan_external_repos.py --help
```

**Prerequisites**:
The external repositories must be cloned as siblings to data-pipeline:

```
GitHub/
├── data-pipeline/           (this repo)
├── community-data/          (external repo - industry data)
└── community-timelines/     (external repo - timeline data)
```

To clone the external repos:
```bash
# From the GitHub directory (parent of data-pipeline)
cd /Users/poojithabommu/Documents/GitHub

# Clone community-data
git clone https://github.com/ModelEarth/community-data.git

# Clone community-timelines
git clone https://github.com/ModelEarth/community-timelines.git
```

**What it tracks**: CSV files written to external repositories (community-data, community-timelines).

**What it updates in nodes.csv**:
- `actual_size_mb` - Total size of CSV files in external repo (MB)
- `csv_file_count` - Number of CSV files in external repo
- `last_transport_date` - Date of last scan

**How it works**:

1. Reads `nodes.csv` to find nodes with `output_path` values
2. For each node, resolves the output_path:
   - If path starts with `../`, resolves relative to the script's `link` directory
   - Checks if resolved path is in an external repo
3. If in external repo, scans the directory for CSV files
4. Updates the node with actual file size and count
5. Saves updated `nodes.csv`

**Example**:

For node `naics_001`:
- `link`: `timelines/prep/industries`
- `output_path`: `../../../../community-timelines/industries/naics4/US/states/`
- Script runs from: `data-pipeline/timelines/prep/industries/`
- Resolves to: `community-timelines/industries/naics4/US/states/`
- Scans that directory for CSV files
- Updates node with actual counts and sizes

### 3. `data_transport.py`

**Purpose**: Original transport script (legacy).

**Status**: Superseded by `data_transport_enhanced.py` which has all the same features plus nodes.csv integration.

## nodes.csv Structure

The `nodes.csv` file tracks all data processing nodes in the pipeline. Each row represents a script, notebook, or process.

### Key Columns

| Column | Description |
|--------|-------------|
| `node_id` | Unique identifier (e.g., `naics_001`) |
| `name` | Human-readable name |
| `description` | What the node does |
| `type` | Type of processor (data_fetcher, data_processor, etc.) |
| `link` | Directory where the script is located |
| `python_cmds` | Command to run the script |
| `output_path` | Where output files are written |
| `actual_size_mb` | **Actual** total size of output files (MB) |
| `csv_file_count` | **Actual** number of CSV files generated |
| `last_transport_date` | Date when files were last scanned/transported |

### Output Path Types

1. **Local paths** (no `../`):
   - Example: `us/zipcodes/`, `states/commodities/2020/`
   - Relative to data-pipeline repo root
   - Tracked by `data_transport_enhanced.py`

2. **External paths** (starts with `../`):
   - Example: `../../../community-data/industries/naics/US/counties-update/`
   - Relative to script's `link` directory
   - Tracked by `scan_external_repos.py`

## Workflow

### Complete Update Workflow

To get a complete picture of all file sizes in nodes.csv:

```bash
# Step 1: Scan external repos (if cloned)
python admin/transport/scan_external_repos.py --verbose

# Step 2: Transport local files and update nodes.csv
python admin/transport/data_transport_enhanced.py

# Result: nodes.csv now has actual_size_mb and csv_file_count for:
#   - Scripts writing to external repos (from step 1)
#   - Scripts writing to local outputs (from step 2)
```

### If External Repos Are Not Cloned

If you haven't cloned community-data and community-timelines:

```bash
# Only run the local transport
python admin/transport/data_transport_enhanced.py

# Result: nodes.csv will have data for local outputs only
# External repo nodes will show empty values for size/count
```

The `scan_external_repos.py` script will warn about missing repos but won't crash.

## Nodes Added for naics-annual.ipynb

The Jupyter notebook `industries/naics/naics-annual.ipynb` was not previously tracked in nodes.csv. Three new entries have been added:

| Node ID | Name | Output Path |
|---------|------|-------------|
| `naics_002` | NAICS Annual County Data Generator | `../../../community-data/industries/naics/US/counties-update/` |
| `naics_003` | NAICS Annual State Data Generator | `../../../community-data/industries/naics/US/states-update/` |
| `naics_004` | NAICS Annual Country Data Generator | `../../../community-data/industries/naics/US/country-update/` |

These nodes write to the community-data external repository and will be tracked by `scan_external_repos.py`.

## Troubleshooting

### "No external repositories found"

**Cause**: The community-data and/or community-timelines repos are not cloned as siblings to data-pipeline.

**Solution**: Clone the repos:
```bash
cd /Users/poojithabommu/Documents/GitHub
git clone https://github.com/ModelEarth/community-data.git
git clone https://github.com/ModelEarth/community-timelines.git
```

### "No CSV files found in directory"

**Possible causes**:
1. The script hasn't been run yet to generate the CSV files
2. The output_path in nodes.csv is incorrect
3. The files were generated but cleaned up/deleted

**Solution**: Run the script to generate the CSV files, or verify the output_path is correct.

### Empty actual_size_mb values in nodes.csv

**Possible causes**:
1. External repos not cloned (for external output nodes)
2. Scripts haven't been run yet to generate files
3. Transport/scan scripts haven't been run

**Solution**:
1. Clone external repos if needed
2. Run the data generation scripts
3. Run `scan_external_repos.py` and/or `data_transport_enhanced.py`

## Development

### Testing

Always test changes with `--dry-run` first:

```bash
# Test external repo scanning
python admin/transport/scan_external_repos.py --dry-run --verbose

# Test local transport
# (Edit script to set OUTPUT_LOCAL_PATH first)
python admin/transport/data_transport_enhanced.py
```

### Adding New Nodes

When adding a new script to data-pipeline:

1. Add entry to `nodes.csv` with all required columns
2. Set `output_path` correctly:
   - Local: `path/relative/to/repo/root/`
   - External: `../../../repo-name/path/` (relative to script's link directory)
3. Run appropriate scan/transport script to populate size/count

### Modifying Retention Rules

To change which files are retained (not transported):

Edit `data_transport_enhanced.py`:

```python
OMIT_CSV_GLOBS = [
    "nodes.csv",
    "*crosswalk*.csv",
    # Add your pattern here
]

OMIT_DIRECTORIES = [
    "timelines/prep/all/input",
    # Add your directory here
]
```

## Files Generated

| File | Description |
|------|-------------|
| `moved-csv.md` | Report of CSV files transported (local copy) |
| `data-pipe-csv/moved-csv.md` | Report in destination repo |
| `data-pipe-csv/node-mapping.md` | Shows which files belong to which nodes |
| `nodes.csv` | Updated with actual file sizes and counts |

## References

- Main issue: [ModelEarth/data-pipeline#38](https://github.com/ModelEarth/data-pipeline/pull/38)
- Investigation findings: [FINDINGS.md](./FINDINGS.md)
- Nodes display: https://model.earth/data-pipeline/admin
- Industry comparison page: https://model.earth/localsite/info/

---

## Legacy Instructions (data_transport.py)

Moves .csv files into a data-pipe-csv sub-folder (repo).
We use to reduce the size of the data-pipeline repo so we can include it in our webroot.

You can fork data-pipe-csv into data-pipeline.
Remove OUTPUT_LOCAL_PATH to push directly to Github.

Run in the root of your local data-pipeline repo:

	python3 -m venv env
	source env/bin/activate
	python admin/transport/data_transport


DELETE_LOCAL_AFTER_COPY = False

Prevents files from being deleted in the data-pipeline folder.