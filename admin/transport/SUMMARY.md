# Data Pipeline Transport Enhancement - Summary

## Mystery Solved ✓

### Where is data being sent to community-data?

**Answer**: The data is being generated by Jupyter notebooks (primarily [naics-annual.ipynb](../../industries/naics/naics-annual.ipynb)) that write directly to a sibling `community-data` repository.

#### Scripts Writing to Community-Data:

1. **[industries/naics/naics-annual.ipynb](../../industries/naics/naics-annual.ipynb)** (Jupyter Notebook)
   - Outputs to: `../../../community-data/industries/naics/US/counties-update/`
   - Outputs to: `../../../community-data/industries/naics/US/states-update/`
   - Outputs to: `../../../community-data/industries/naics/US/country-update/`
   - Outputs to: `../../../community-data/us/id_lists/industry_ID_list.csv`
   - **Not currently tracked in [nodes.csv](../../nodes.csv)** because it's a notebook, not a Python script

2. **[timelines/prep/industries/naics-timelines.py](../../timelines/prep/industries/naics-timelines.py:8)** (Python Script)
   - Outputs to: `../../../../community-timelines/industries/naics4/US/states/`
   - **IS tracked** in nodes.csv as `naics_001` ✓

### Data Used by model.earth/localsite/info/

The industry comparison page uses data from:
- `https://model.earth/community-data/industries/naics/US/counties/`
- `https://model.earth/community-data/industries/naics/US/states/`
- `https://model.earth/community-data/us/id_lists/industry_id_list.csv`

This data is generated by the `naics-annual.ipynb` notebook, then manually moved from the `-update` folders to the final locations.

---

## What We Built

### Enhanced Transport Script

Created [data_transport_enhanced.py](data_transport_enhanced.py) with these new features:

1. **Automatic nodes.csv Updates**
   - Matches transported CSV files to nodes based on `output_path` and `link` columns
   - Calculates total file sizes and counts per node
   - Adds three new columns to nodes.csv:
     - `actual_size_mb`: Total size of all CSV files (in MB)
     - `csv_file_count`: Number of CSV files transported
     - `last_transport_date`: Date of last transport

2. **Enhanced File Retention Rules**
   - Automatically keeps lookup/crosswalk files in the source repo
   - Skips entire input data directories
   - Patterns retained:
     - `*crosswalk*.csv`, `*_to_*.csv`, `*fips*.csv`, `*id_list*.csv`, `*lookup*.csv`
     - All files in `timelines/prep/all/input/`

3. **Node Mapping Report**
   - New file: `node-mapping.md` shows which CSV files belong to which nodes
   - Helps verify matching logic is working correctly

---

## Test Results

### Files Processed
- **83 files moved** to data-pipe-csv/
- **39 files retained** in data-pipeline (lookups, inputs, nodes.csv)
- **2 nodes matched** with actual data

### Successfully Matched Nodes

#### eco_001: Economic Data Fetcher
- **Files**: 51 CSV files
- **Total Size**: 0.66 MB (671.2 KB)
- **Location**: `research/economy/states/commodities/2020/`
- **Updated in nodes.csv**: `actual_size_mb=0.66, csv_file_count=51, last_transport_date=2025-11-19`

#### reg_001: Time Series Data Cleaner
- **Files**: 14 CSV files
- **Total Size**: 78.48 MB
- **Location**: `timelines/prep/regression/data/`
- **Updated in nodes.csv**: `actual_size_mb=78.48, csv_file_count=14, last_transport_date=2025-11-19`

---

## Files Retained (Not Transported)

The enhanced script automatically retained these files in the data-pipeline repo:

### Registry Files
- [nodes.csv](../../nodes.csv) - The node registry itself

### Input Data (39 files total)
- `timelines/prep/all/input/` directory (all subdirectories)
  - 2011-2017 ZCTA industry data
  - Poverty data from American Community Survey
  - Summary statistics
  - Random forest poverty predictions

These are **input dependencies** for ML/regression scripts and must stay in the repo.

---

## Generated Reports

### 1. [moved-csv.md](../../data-pipe-csv/moved-csv.md)
Transport report showing:
- All moved files grouped by folder
- File sizes
- Retained files list
- Top 10 largest remaining files

### 2. [node-mapping.md](../../data-pipe-csv/node-mapping.md)
Node-to-file mapping showing:
- Which CSV files matched to which nodes
- File counts and sizes per node
- Helps verify the matching logic

### 3. Updated [nodes.csv](../../nodes.csv)
Now includes three new columns:
- `actual_size_mb`
- `csv_file_count`
- `last_transport_date`

---

## How the Matching Works

The script uses a flexible 3-strategy approach to match files to nodes:

1. **Direct Prefix Match**: File path starts with `output_path`
   ```
   output_path: "states/commodities/2020/"
   matches: "states/commodities/2020/CA.csv" ✓
   ```

2. **Combined Path Match**: File path starts with `link` + `output_path`
   ```
   link: "research/economy"
   output_path: "states/commodities/2020/"
   matches: "research/economy/states/commodities/2020/CA.csv" ✓
   ```

3. **Partial Path Match**: `output_path` is contained within file path
   ```
   output_path: "data/"
   matches: "timelines/prep/regression/data/combo.csv" ✓
   ```

This flexible matching allows for different folder structures and relative paths.

---

## Why Some Nodes Didn't Match

Many nodes in [nodes.csv](../../nodes.csv) show `actual_size_mb` and `csv_file_count` as empty because:

1. **No CSV files exist** at those output paths (scripts haven't been run yet)
2. **Files were manually moved** to community-data repository
3. **Output paths are placeholders** or relative to different repos
4. **Jupyter notebooks** aren't currently tracked in nodes.csv

Examples:
- `zip_001`, `zip_002`, `zip_003` - No zipcodes data in current repo
- `naics_001` - Outputs to community-timelines (different repo)
- `ml_*`, `pov_*`, `train_*` - Output files may not exist locally

---

## Next Steps

### Immediate Actions

1. **Review the test results**:
   ```bash
   # Check the updated nodes.csv
   cat nodes.csv | grep -E "(eco_001|reg_001)"

   # Review the node mapping
   cat data-pipe-csv/node-mapping.md

   # Check retained files
   cat data-pipe-csv/moved-csv.md | grep -A 50 "Retained CSVs"
   ```

2. **Decide on deployment**:
   - Currently running in LOCAL mode (outputs to `data-pipe-csv/` folder)
   - To push to GitHub: Set `OUTPUT_LOCAL_PATH = None` in the script

3. **Add missing nodes**:
   - Consider adding `naics-annual.ipynb` to nodes.csv
   - Track other Jupyter notebooks that generate data

### Optional Enhancements

1. **Add validation** to detect unmatched files (files that don't belong to any node)
2. **Auto-detect Jupyter notebooks** and add them to nodes.csv
3. **Track destination repos** (community-data vs community-timelines vs data-pipe-csv)
4. **Add file hash tracking** to detect when files change

---

## Documentation Created

1. **[FINDINGS.md](FINDINGS.md)** - Investigation results and mystery solution
2. **[ENHANCED_README.md](ENHANCED_README.md)** - Complete guide to using the enhanced script
3. **[data_transport_enhanced.py](data_transport_enhanced.py)** - The enhanced transport script
4. **[SUMMARY.md](SUMMARY.md)** - This file

---

## Configuration Files

### Key Settings in data_transport_enhanced.py

```python
# Enable/disable nodes.csv updates
UPDATE_NODES_CSV = True

# Local output mode (set to None for GitHub mode)
OUTPUT_LOCAL_PATH = "data-pipe-csv"

# File patterns to retain (not transport)
OMIT_CSV_GLOBS = [
    "node.csv",
    "nodes.csv",
    "*crosswalk*.csv",
    "*_to_*.csv",
    "*fips*.csv",
    "*id_list*.csv",
    "*lookup*.csv",
]

# Directories to skip entirely
OMIT_DIRECTORIES = [
    "timelines/prep/all/input",
]
```

---

## Questions & Answers

**Q: Why are most nodes showing empty values for the new columns?**
A: Either the CSV files don't exist locally, or they've been moved to other repos (community-data, community-timelines).

**Q: How do I add more retention rules?**
A: Edit `OMIT_CSV_GLOBS` or `OMIT_DIRECTORIES` in the enhanced script.

**Q: Can I track Jupyter notebooks in nodes.csv?**
A: Yes! Manually add entries for them with appropriate `output_path` values.

**Q: What if a node has multiple output paths?**
A: Currently, nodes.csv supports one `output_path` per node. Consider creating multiple node entries or using a more general path that covers all outputs.

**Q: How do I reset and test again?**
A: Delete the `data-pipe-csv/` folder and run the script again:
```bash
rm -rf data-pipe-csv/
python admin/transport/data_transport_enhanced.py
```

---

## Success Metrics

✅ Solved the mystery of where community-data files come from
✅ Created enhanced transport script with nodes.csv integration
✅ Successfully matched 2 nodes with 65 CSV files (79.14 MB total)
✅ Automatically retained 39 input/lookup files
✅ Generated comprehensive reports and documentation
✅ Added 3 new tracking columns to nodes.csv

---

## Contact & Support

For questions or issues:
- Review [ENHANCED_README.md](ENHANCED_README.md) for detailed usage guide
- Check [FINDINGS.md](FINDINGS.md) for investigation details
- Examine generated reports in `data-pipe-csv/` folder
