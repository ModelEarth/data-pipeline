# Community-Data Output Path Investigation

## Mystery Solved: Scripts Outputting to Community-Data

### Summary
The mystery is that several Jupyter notebooks and Python scripts are writing directly to the `community-data` repository (which is expected to be a sibling directory to `data-pipeline`), but these output paths are **not reflected** in [nodes.csv](../../nodes.csv).

### Scripts Writing to Community-Data

#### 1. **naics-annual.ipynb** (Jupyter Notebook)
**Location**: [industries/naics/naics-annual.ipynb](../../industries/naics/naics-annual.ipynb)

**Output Paths**:
- `../../../community-data/us/id_lists/industry_ID_list.csv` (line 2128)
- `../../../community-data/industries/naics/US/counties-update/{stateName}/{filename}` (line 4002)
- `../../../community-data/industries/naics/US/states-update/{stateName}/{filename}` (line 6135)
- `../../../community-data/industries/naics/US/country-update/{filename}` (line 6205)

**Description**: This notebook generates NAICS industry data for counties, states, and country levels using Census Bureau API data.

**Node ID in nodes.csv**: Not currently tracked! This is a Jupyter notebook, not a Python script.

---

#### 2. **naics-timelines.py** (Python Script)
**Location**: [timelines/prep/industries/naics-timelines.py](../../timelines/prep/industries/naics-timelines.py:8)

**Output Paths**:
- `../../../../community-timelines/industries/naics4/US/states/` (aggregated state-level timeline data)

**Description**: Aggregates NAICS data across years (2017-2020) to create timeline datasets by fetching from community-data and outputting to community-timelines.

**Node ID in nodes.csv**: `naics_001` - This one IS tracked in nodes.csv!

---

#### 3. **ML_data_generation.py** (Python Script)
**Location**: [timelines/training/naics/python/ML_data_generation.py](../../timelines/training/naics/python/ML_data_generation.py)

**Input Data Source**: Fetches from `https://model.earth/community-data/industries/naics/US/counties/{state}/...`

**Output Path**: `output/NAICS{level}/{year}/` (local output, then likely manually moved to community-data)

**Node ID in nodes.csv**: `train_002`

---

### Data Sources Used by model.earth/localsite/info/

The industry comparison page at https://model.earth/localsite/info/ uses data from:
- `https://model.earth/community-data/industries/naics/US/counties/`
- `https://model.earth/community-data/industries/naics/US/states/`
- `https://model.earth/community-data/us/id_lists/industry_id_list.csv`

These are generated by the `naics-annual.ipynb` notebook.

---

### CSV Files Used as Lookups/Crosswalks (Should Be Retained)

The following CSV files are **input dependencies** for other scripts and should NOT be moved by the transport script:

1. **node.csv** (already exempted in transport script)
   - Used by: admin dashboard display

2. **Crosswalk files** (if they exist):
   - Zip-to-ZCTA crosswalk files (referenced in zipgraph.py scripts)
   - Pattern: `*crosswalk*.csv`, `*_to_*.csv`

3. **ID Lists and FIPS codes**:
   - `state_fips.csv` (referenced in naics-annual.ipynb)
   - `industry_id_list.csv` or similar lookup files
   - Pattern: `*fips*.csv`, `*id_list*.csv`

4. **Input files for ML/regression**:
   - Files in `timelines/prep/all/input/` directory
   - Community forecasting ZCTA data files read by poverty.py

---

### Recommendations

1. **Add naics-annual.ipynb to nodes.csv**
   - This is a major data generator but not currently tracked
   - Should have entries for county-level, state-level, and country-level outputs

2. **Update output_path column in nodes.csv**
   - Many entries show local paths (`us/zipcodes/`, `states/commodities/2020/`)
   - Should be updated to reflect actual community-data paths if data was manually moved

3. **Enhance transport script to:**
   - Scan for actual output CSV files
   - Match them to nodes.csv entries by folder structure
   - Calculate file sizes
   - Update nodes.csv with actual destination paths and sizes
   - Exempt lookup/crosswalk files from transport

4. **Add new columns to nodes.csv**:
   - `actual_output_path`: Where files actually end up (community-data, community-timelines, etc.)
   - `file_size`: Total size of output files
   - `file_count`: Number of CSV files generated
   - `last_modified`: Most recent modification date of output files

---

### Next Steps

1. Modify `data_transport.py` to:
   - Read nodes.csv
   - For each CSV file being transported, try to match it to a node entry
   - Update nodes.csv with actual paths and sizes
   - Generate a mapping report showing which files belong to which nodes

2. Create exemption rules for:
   - `node.csv`
   - Files matching `*crosswalk*.csv`, `*_to_*.csv`
   - Files matching `*fips*.csv`, `*id_list*.csv`
   - Files in `timelines/prep/all/input/` directory

3. Test the enhanced script in LOCAL_OUTPUT mode first before pushing to GitHub
