#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
External Repository Scanner for nodes.csv

This script scans external repositories (community-data, community-timelines) to find
CSV files generated by data-pipeline scripts and updates nodes.csv with actual file sizes
and counts from those external repos.

Key features:
1. Handles relative paths like ../../../../community-timelines/...
2. Resolves paths correctly from the context of where each script runs (the link column)
3. Scans sibling repos that may or may not be cloned locally
4. Gracefully handles missing repos - warns the user but doesn't crash
5. Reuses matching logic similar to data_transport_enhanced.py
6. Supports Jupyter notebooks like naics-annual.ipynb

Usage:
    python admin/transport/scan_external_repos.py [--dry-run] [--verbose]
"""

import os
import sys
import csv
import json
from pathlib import Path
from datetime import datetime, timezone
from collections import defaultdict
from typing import Dict, List, Tuple, Optional

# ========== CONFIG ==========
NODES_CSV_PATH = "nodes.csv"
DATA_PIPELINE_ROOT = Path(__file__).resolve().parents[2]  # Root of data-pipeline repo
GITHUB_ROOT = DATA_PIPELINE_ROOT.parent  # Parent directory containing sibling repos

# External repos to scan
EXTERNAL_REPOS = {
    "community-data": GITHUB_ROOT / "community-data",
    "community-timelines": GITHUB_ROOT / "community-timelines"
}

# Verbose output
VERBOSE = False

# ========== UTILITIES ==========
def human_size(nbytes: int) -> str:
    """Convert file size from bytes to human readable size."""
    units = ["B", "KB", "MB", "GB", "TB"]
    size = float(nbytes)
    for u in units:
        if size < 1024 or u == "TB":
            return f"{size:.1f} {u}"
        size /= 1024.0

def bytes_to_mb(nbytes: int) -> float:
    """Convert bytes to megabytes."""
    return round(nbytes / (1024 * 1024), 2)

def log(msg: str, verbose_only: bool = False):
    """Print log message."""
    if not verbose_only or VERBOSE:
        print(msg)

def normalize_path(path_str: str) -> str:
    """Normalize path by removing leading/trailing slashes and converting to posix."""
    return path_str.strip().strip("/").replace("\\", "/")

# ========== PATH RESOLUTION ==========
def resolve_output_path(output_path: str, script_link: str, repo_root: Path) -> Optional[Path]:
    """
    Resolve the actual filesystem path for an output_path that may contain relative references.

    Args:
        output_path: The output_path from nodes.csv (e.g., "../../../../community-timelines/...")
        script_link: The link column from nodes.csv (e.g., "timelines/prep/industries")
        repo_root: Root of the data-pipeline repository

    Returns:
        Resolved absolute Path object, or None if it can't be resolved
    """
    output_path = normalize_path(output_path)

    # If the path starts with ../, it's relative to the script location
    if output_path.startswith("../"):
        # The script runs from repo_root / script_link
        script_dir = repo_root / script_link
        # Resolve the relative path from the script directory
        try:
            resolved = (script_dir / output_path).resolve()
            return resolved
        except Exception as e:
            log(f"[WARN] Could not resolve path {output_path} from {script_dir}: {e}", verbose_only=True)
            return None

    # If it's an absolute path or doesn't start with ../, treat it as relative to repo root
    else:
        return repo_root / output_path

def is_external_repo_path(resolved_path: Path) -> Tuple[bool, Optional[str]]:
    """
    Check if a resolved path is in an external repository.

    Returns:
        (is_external, repo_name) where repo_name is "community-data", "community-timelines", etc.
    """
    try:
        for repo_name, repo_path in EXTERNAL_REPOS.items():
            if repo_path.exists():
                # Check if resolved_path is within repo_path
                try:
                    resolved_path.relative_to(repo_path)
                    return (True, repo_name)
                except ValueError:
                    continue
    except Exception as e:
        log(f"[WARN] Error checking external repo path: {e}", verbose_only=True)

    return (False, None)

# ========== FILE SCANNING ==========
def scan_directory_for_csvs(directory: Path, repo_name: str) -> Tuple[List[Tuple[str, int]], int, int]:
    """
    Scan a directory for CSV files and collect their sizes.

    Args:
        directory: Directory to scan
        repo_name: Name of the external repo (for logging)

    Returns:
        (files_list, total_size, file_count) where files_list is [(rel_path, size), ...]
    """
    if not directory.exists():
        log(f"[WARN] Directory does not exist: {directory}")
        return ([], 0, 0)

    files = []
    total_size = 0

    try:
        for csv_file in directory.rglob("*.csv"):
            if csv_file.is_file():
                try:
                    size = csv_file.stat().st_size
                    rel_path = str(csv_file.relative_to(directory))
                    files.append((rel_path, size))
                    total_size += size
                except Exception as e:
                    log(f"[WARN] Could not stat file {csv_file}: {e}", verbose_only=True)
    except Exception as e:
        log(f"[WARN] Error scanning directory {directory}: {e}")

    file_count = len(files)

    if file_count > 0:
        log(f"[SCAN] Found {file_count} CSV files ({human_size(total_size)}) in {repo_name}: {directory}", verbose_only=True)

    return (files, total_size, file_count)

# ========== NODES.CSV PROCESSING ==========
def load_nodes_csv(nodes_path: Path) -> Tuple[List[Dict], List[str]]:
    """Load nodes.csv and return as list of dicts and fieldnames."""
    if not nodes_path.exists():
        raise FileNotFoundError(f"nodes.csv not found at {nodes_path}")

    nodes = []
    with open(nodes_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames
        for row in reader:
            nodes.append(row)

    return (nodes, fieldnames)

def save_nodes_csv(nodes_path: Path, nodes: List[Dict], fieldnames: List[str]):
    """Save updated nodes.csv."""
    with open(nodes_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(nodes)

    log(f"[SAVED] Updated {nodes_path}")

def update_node_with_external_data(node: Dict, total_size: int, file_count: int, transport_date: str):
    """Update a node dict with external repo data."""
    # Initialize columns if they don't exist
    if "actual_size_mb" not in node:
        node["actual_size_mb"] = ""
    if "csv_file_count" not in node:
        node["csv_file_count"] = ""
    if "last_transport_date" not in node:
        node["last_transport_date"] = ""

    # Update with actual values
    if file_count > 0:
        node["actual_size_mb"] = bytes_to_mb(total_size)
        node["csv_file_count"] = file_count
        node["last_transport_date"] = transport_date

# ========== MAIN LOGIC ==========
def scan_external_repos_for_nodes(dry_run: bool = False):
    """
    Main function to scan external repos and update nodes.csv.
    """
    log("=" * 80)
    log("External Repository Scanner")
    log("=" * 80)

    # Check which external repos are available
    available_repos = {}
    missing_repos = []

    for repo_name, repo_path in EXTERNAL_REPOS.items():
        if repo_path.exists() and repo_path.is_dir():
            available_repos[repo_name] = repo_path
            log(f"[OK] Found external repo: {repo_name} at {repo_path}")
        else:
            missing_repos.append(repo_name)
            log(f"[WARN] External repo not found: {repo_name} (expected at {repo_path})")

    if not available_repos:
        log("\n[ERROR] No external repositories found. Cannot proceed.")
        log("Expected repos at:")
        for repo_name, repo_path in EXTERNAL_REPOS.items():
            log(f"  - {repo_name}: {repo_path}")
        return

    # Load nodes.csv
    nodes_path = DATA_PIPELINE_ROOT / NODES_CSV_PATH
    log(f"\n[LOADING] nodes.csv from {nodes_path}")

    try:
        nodes, fieldnames = load_nodes_csv(nodes_path)
    except Exception as e:
        log(f"[ERROR] Failed to load nodes.csv: {e}")
        return

    # Ensure new columns exist in fieldnames
    new_columns = ["actual_size_mb", "csv_file_count", "last_transport_date"]
    for col in new_columns:
        if col not in fieldnames:
            fieldnames.append(col)

    log(f"[LOADED] {len(nodes)} nodes from nodes.csv")

    # Current date for transport_date
    transport_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Process each node
    updated_count = 0
    skipped_count = 0

    log("\n" + "=" * 80)
    log("Processing Nodes")
    log("=" * 80)

    for node in nodes:
        node_id = node.get("node_id", "")
        node_name = node.get("name", "Unknown")
        output_path = node.get("output_path", "").strip()
        link = node.get("link", "").strip()

        if not output_path or not node_id:
            skipped_count += 1
            continue

        log(f"\n[NODE] {node_id}: {node_name}")
        log(f"  output_path: {output_path}")
        log(f"  link: {link}")

        # Resolve the output path
        resolved_path = resolve_output_path(output_path, link, DATA_PIPELINE_ROOT)

        if not resolved_path:
            log(f"  [SKIP] Could not resolve output path")
            skipped_count += 1
            continue

        log(f"  resolved_path: {resolved_path}", verbose_only=True)

        # Check if it's in an external repo
        is_external, repo_name = is_external_repo_path(resolved_path)

        if not is_external:
            log(f"  [SKIP] Not in external repo (local output)")
            skipped_count += 1
            continue

        log(f"  [EXTERNAL] In repo: {repo_name}")

        # Check if the repo is available
        if repo_name not in available_repos:
            log(f"  [WARN] Repo {repo_name} not cloned locally - skipping")
            skipped_count += 1
            continue

        # Scan the directory for CSV files
        files, total_size, file_count = scan_directory_for_csvs(resolved_path, repo_name)

        if file_count > 0:
            log(f"  [FOUND] {file_count} CSV files, {human_size(total_size)} total")

            # Update the node
            update_node_with_external_data(node, total_size, file_count, transport_date)
            updated_count += 1

            # Show some sample files if verbose
            if VERBOSE and files:
                log(f"  Sample files:")
                for rel_path, size in files[:5]:
                    log(f"    - {rel_path} ({human_size(size)})")
                if len(files) > 5:
                    log(f"    ... and {len(files) - 5} more")
        else:
            log(f"  [EMPTY] No CSV files found in directory")
            skipped_count += 1

    # Summary
    log("\n" + "=" * 80)
    log("Summary")
    log("=" * 80)
    log(f"Nodes processed: {len(nodes)}")
    log(f"Nodes updated: {updated_count}")
    log(f"Nodes skipped: {skipped_count}")

    if missing_repos:
        log(f"\nMissing repos: {', '.join(missing_repos)}")
        log("Note: These repos should be cloned as siblings to data-pipeline for full scanning.")

    # Save updated nodes.csv
    if not dry_run:
        if updated_count > 0:
            save_nodes_csv(nodes_path, nodes, fieldnames)
            log(f"\n[SUCCESS] Updated nodes.csv with external repo data")
        else:
            log(f"\n[INFO] No updates to save (no external repo files found)")
    else:
        log(f"\n[DRY-RUN] Would have updated {updated_count} nodes")

# ========== CLI ==========
def main():
    global VERBOSE

    # Parse command line arguments
    dry_run = "--dry-run" in sys.argv
    VERBOSE = "--verbose" in sys.argv or "-v" in sys.argv

    if "--help" in sys.argv or "-h" in sys.argv:
        print(__doc__)
        print("\nOptions:")
        print("  --dry-run    Show what would be updated without saving")
        print("  --verbose    Show detailed output")
        print("  -v           Same as --verbose")
        print("  --help       Show this help message")
        return

    try:
        scan_external_repos_for_nodes(dry_run=dry_run)
    except Exception as e:
        log(f"\n[ERROR] {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
